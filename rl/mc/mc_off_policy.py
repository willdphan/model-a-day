"""
Monte Carlo Off-Policy with Importance Sampling
Created: 03-03-2024
---
This simplified script simulates episodes using a random walk as the behavior policy and evaluates the rewards under a deterministic target policy aiming to minimize the distance to the origin. 

The script simulates random walks to evaluate a target policy's effectiveness in minimizing distance to the origin using importance sampling. It generates episodes based on a random behavior policy, calculates rewards favoring closeness to the origin, and then evaluates the target policy by comparing chosen actions against those the target policy would prefer. 

Importance sampling adjusts rewards based on the likelihood of actions under both policies, allowing for an unbiased evaluation of the target policy's performance. The process concludes by averaging the adjusted rewards across all simulations to assess the average effectiveness of the target policy.

The average reward of approximately -3.44 suggests that, on average, the target policy, when evaluated under these conditions, performs poorly compared to the behavior policy.
"""

import random

def random_walk_behavior_policy(n):
    """Generates an episode using the behavior policy (random walk)."""
    episode = []
    x, y = 0, 0
    for _ in range(n):
        action = random.choice([(0, 1), (0, -1), (1, 0), (-1, 0)])
        next_x, next_y = x + action[0], y + action[1]
        # Assume a reward that favors being closer to the origin
        reward = -abs(next_x) - abs(next_y)  # More negative the farther from origin
        episode.append(((x, y), action, reward, (next_x, next_y)))
        x, y = next_x, next_y
    return episode

def target_policy_action(state):
    """Defines the target policy's preferred action given the current state."""
    x, y = state
    # Hypothetical target policy prefers actions that minimize the Manhattan distance to the origin
    if abs(x) > abs(y):
        return (-1, 0) if x > 0 else (1, 0)
    else:
        return (0, -1) if y > 0 else (0, 1)

def evaluate_target_policy_with_importance_sampling(episode):
    """Evaluates the performance of the target policy using importance sampling."""
    total_weighted_reward = 0
    total_importance_ratio = 0
    for step in episode:
        state, behavior_action, reward, _ = step
        target_action = target_policy_action(state)
        # Importance sampling ratio: P(target policy)/P(behavior policy)
        # Assuming equal probability for all actions under the behavior policy (0.25)
        # and deterministic target policy (probability of 1 for the chosen action)
        importance_ratio = 1 / 0.25 if behavior_action == target_action else 0
        total_weighted_reward += reward * importance_ratio
        total_importance_ratio += importance_ratio
    
    # Normalize the weighted rewards by the total importance ratio to get the average reward
    average_reward = total_weighted_reward / total_importance_ratio if total_importance_ratio > 0 else 0
    return average_reward

number_of_walks = 20000
walk_length = 30  # Fixed walk length for simplification

# Collect rewards from episodes generated by the behavior policy
total_rewards = 0
for _ in range(number_of_walks):
    episode = random_walk_behavior_policy(walk_length)
    reward = evaluate_target_policy_with_importance_sampling(episode)
    total_rewards += reward

# Average reward per walk
average_reward = total_rewards / number_of_walks
print(f"Average reward under the target policy (evaluated using behavior policy episodes with importance sampling): {average_reward}")
